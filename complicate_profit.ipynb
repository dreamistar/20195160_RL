{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "from pathlib import Path\n",
    "import collections\n",
    "import sys\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta, date\n",
    "from sklearn.preprocessing import PolynomialFeatures as pf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment\n",
    "- 4 Types of features\n",
    "    - Demand : user-defined range / continuous / # of demand of the product\n",
    "    - inventory : no limiations / continuous / # of remaining product in inventory\n",
    "    - defeat : [0, 1] / about 10 ~ 100 cases / % of defeat of certain machine\n",
    "    - check : 0 or 1 / binary / if the defeat of machine is over certain %, it should be checked\n",
    "- with polynomial linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMachine : \n",
    "    def __init__(self, machine_params) : \n",
    "        self.machine_params = machine_params\n",
    "        self.defeat_prop = machine_params['defeat_prop']\n",
    "        self.checking = 0\n",
    "        self.made_list, self.defeat_list = [], []\n",
    "        \n",
    "    def make(self, num) : \n",
    "        made = round(num * (1 - self.defeat_prop))        \n",
    "        return made\n",
    "    \n",
    "\"\"\"\n",
    "1. initialize : set machine, get demend\n",
    "2. do action \n",
    "3. make product : update made, update machine params\n",
    "4. get reward \n",
    "5. update inventory \n",
    "6. update demand \n",
    "7. update state\n",
    "\"\"\"    \n",
    "class CustomManufacturingProcess : \n",
    "    def __init__(self, process_params, machine_params) : \n",
    "        self.process_params, self.machine_params = process_params, machine_params\n",
    "        \n",
    "        self.demand, self.not_satisfied, self.inventory, self.made, self.day = self.get_demand(init = True), 0, 0, 0, 1\n",
    "        self.demand_list, self.not_satisfied_list, self.inventory_list, self.made_list = [], [], [], []\n",
    "        \n",
    "        self.machines_list = [CustomMachine(machine_params[k]) for k in list(machine_params.keys())]\n",
    "        self.state = {'demand' : self.demand, 'inventory' : self.inventory}\n",
    "    \n",
    "    def get_demand(self, init = False) :\n",
    "        if init == True : \n",
    "            demand = self.process_params['demand_mu']\n",
    "        else : \n",
    "            demand = np.random.normal(self.process_params['demand_mu'], self.process_params['demand_sigma'])\n",
    "            if demand < self.process_params['demand_space'][0] : demand = self.process_params['demand_space'][0]\n",
    "            elif demand > self.process_params['demand_space'][-1] : demand = self.process_params['demand_space'][-1]\n",
    "        return round(demand)\n",
    "    \n",
    "    def make_product(self, action, save = True) : \n",
    "        under = 1/np.sum([1/machine.defeat_prop for machine in self.machines_list])\n",
    "        current_made = [round(machine.make((1/machine.defeat_prop)* (action/under))) for machine in self.machines_list]\n",
    "        self.made = int(np.sum(current_made))\n",
    "            \n",
    "    def get_reward(self, action) : \n",
    "        self.make_product(action)\n",
    "        total_profit = np.min([self.demand, self.made + self.inventory]) * self.process_params['sell_price'] - self.made * self.process_params['make_cost']\n",
    "        self.not_satisfied = np.max([self.demand - self.made - self.inventory, 0])\n",
    "        self.not_satisfied_list.append(self.not_satisfied)\n",
    "        self.inventory = np.max([0, self.inventory + self.made - self.demand])      \n",
    "        total_profit -= self.inventory * self.process_params['inventory_cost']\n",
    "        if self.inventory > self.process_params['inventory_space'][-1] : self.inventory = self.process_params['inventory_space'][-1]\n",
    "        self.state['inventory'] = self.inventory\n",
    "        self.inventory_list.append(self.inventory)\n",
    "        if self.day == self.process_params['max_day'] : \n",
    "            return total_profit - self.not_satisfied * self.process_params['not_satisfied'], True\n",
    "        else : \n",
    "            return total_profit - self.not_satisfied * self.process_params['not_satisfied'], False \n",
    "        \n",
    "    def make_state(self) : \n",
    "        self.demand = self.get_demand() # demand update\n",
    "        self.demand_list.append(self.demand)\n",
    "        self.state['demand'] = self.demand + self.not_satisfied\n",
    "        \n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 30000\n",
    "#max_epoch/10\n",
    "policy_params = {'alpha' : 1e-3, 'gamma' : 0.95, 'epsilon' : 0.95,\n",
    "                 'max_epoch' : max_epoch, \n",
    "                 #'verbose' : max_epoch/10}\n",
    "                 'verbose' : None}\n",
    "process_params = {'action_space' : [0, 50], \n",
    "                  'inventory_space' : [0, 100],\n",
    "                  'demand_space' : [10, 70], 'demand_mu' : 30, 'demand_sigma' : 5,\n",
    "                  'sell_price' : 10, 'make_cost' : 5, 'inventory_cost' : 1, 'not_satisfied' : 10, 'max_day' : 20}\n",
    "machine_params = {1 : {'defeat_prop' : 0.01}, 2 : {'defeat_prop' : 0.03}}\n",
    "environment = CustomManufacturingProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSARSA : \n",
    "    def __init__(self, policy_params, process_params, machine_params, environment) : \n",
    "        self.policy_params, self.process_params, self.machine_params, self.environment = policy_params, process_params, machine_params, environment\n",
    "        self.reward_list, self.action_list, self.demand_list, self.not_list, self.inventory_list = [], [], [], [], []\n",
    "        \n",
    "        self.environment = environment\n",
    "        self.env = self.environment(self.process_params, self.machine_params) \n",
    "        \n",
    "        self.gamma, self.alpha, self.epsilon = self.policy_params['gamma'], self.policy_params['alpha'], self.policy_params['epsilon']\n",
    "        self.R_ba, self.q = 0, np.zeros((self.process_params['demand_space'][-1] * self.process_params['demand_space'][-1] + 1, \n",
    "                                         self.process_params['inventory_space'][-1] + 1, \n",
    "                                         self.process_params['max_day'] + 1,\n",
    "                                         self.process_params['action_space'][-1] + 1))\n",
    "    \n",
    "    def state_action_loc(self, S, A) : \n",
    "        return S['demand'], S['inventory'], A\n",
    "        \n",
    "    def state_value(self, S, A, optim = False) :\n",
    "        demand_loc, inventory_loc, action_loc = self.state_action_loc(S, A)\n",
    "        if optim : q = self.q[(demand_loc, inventory_loc, self.env.day + 1, action_loc)]\n",
    "        else : q = self.q[(demand_loc, inventory_loc, self.env.day, action_loc)]\n",
    "        return q\n",
    "    \n",
    "    def initialize(self) : \n",
    "        self.env = self.environment(self.process_params, self.machine_params)\n",
    "        return self.env.state\n",
    "    \n",
    "    def actor(self, S, optim = False) : \n",
    "        p = rd.random()\n",
    "        if p > self.epsilon : \n",
    "            return rd.randrange(self.process_params['action_space'][0], self.process_params['action_space'][-1])\n",
    "        elif not optim : \n",
    "            reward_list = [self.state_value(S, action) for action in range(self.process_params['action_space'][0], self.process_params['action_space'][-1])]\n",
    "            return np.argmax(reward_list)\n",
    "        else : \n",
    "            reward_list = [self.state_value(S, action, optim = True) for action in range(self.process_params['action_space'][0], self.process_params['action_space'][-1])]\n",
    "            return np.argmax(reward_list)\n",
    "    \n",
    "    def fit(self) : \n",
    "        for epoch in tqdm(range(0, self.policy_params['max_epoch'])) : \n",
    "            S = self.initialize()\n",
    "            A = self.actor(S)\n",
    "            current_reward, current_action, current_demand, current_inventory, current_not = [], [], [], [], []\n",
    "            stop = False\n",
    "            while not stop :\n",
    "                demand_loc, inventory_loc, action_loc = self.state_action_loc(S, A)\n",
    "                \n",
    "                if self.policy_params['verbose'] != None : \n",
    "                    if epoch%self.policy_params['verbose'] == 0 : \n",
    "                        print('Epoch {} | i {}'.format(epoch, self.env.day))\n",
    "                        print('Current State | demand {} | inventory {}'.format(self.env.demand, self.env.inventory))   \n",
    "                \n",
    "                R, stop = self.env.get_reward(A)\n",
    "                current_reward.append(R), current_action.append(A), current_demand.append(S['demand'])\n",
    "                \n",
    "                new_S = self.env.make_state()\n",
    "                \n",
    "                if self.policy_params['verbose'] != None : \n",
    "                    if epoch%self.policy_params['verbose'] == 0 : \n",
    "                        print('Result | Action {} | made {} | Reward {}'.format(A, self.env.made, R))\n",
    "                        print('Next State | demand {} | inventory {}'.format(self.env.demand, self.env.inventory))     \n",
    "                        \n",
    "                if stop : delta = R \n",
    "                else : \n",
    "                    new_A = self.actor(new_S, optim = True)\n",
    "                    delta = R + self.gamma * self.state_value(new_S, new_A, optim = True) - self.state_value(S, A)\n",
    "                    \n",
    "                if self.policy_params['verbose'] != None : \n",
    "                    if epoch%self.policy_params['verbose'] == 0 : \n",
    "                        if stop : print('next value {} | value {}'.format(0, self.state_value(S, A)))\n",
    "                        else : print('next value {} | value {}'.format(self.state_value(new_S, ), self.state_value(S, A)))\n",
    "                        print('before : ', self.q[(demand_loc, inventory_loc, self.env.day)])\n",
    "                self.q[(demand_loc, inventory_loc, self.env.day, action_loc)] += self.alpha * delta\n",
    "                if self.policy_params['verbose'] != None : \n",
    "                    if epoch%self.policy_params['verbose'] == 0 : \n",
    "                        print('action {} | plus {}'.format(action_loc, self.alpha * delta))\n",
    "                        print('after : ', self.q[(demand_loc, inventory_loc, self.env.day)])\n",
    "                        print('=' * 40)\n",
    "                #self.q[(demand_loc, inventory_loc, action_loc)] += self.alpha * delta\n",
    "                #print(self.q)\n",
    "                \n",
    "                S = new_S\n",
    "                A = new_A\n",
    "                self.env.day += 1\n",
    "                \n",
    "            self.reward_list.append(current_reward), self.action_list.append(current_action), self.demand_list.append(current_demand), \n",
    "            self.not_list.append(np.mean(self.env.not_satisfied_list)), self.inventory_list.append(np.mean(self.env.inventory_list))\n",
    "            \n",
    "            if self.policy_params['verbose'] != None : \n",
    "                if epoch%self.policy_params['verbose'] == 0 : \n",
    "                    print(\"R mean {} | total action {} | total demand {}\".format(np.mean(current_reward), np.sum(current_action), np.sum(current_demand)))\n",
    "                    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [01:25<00:00, 352.54it/s]\n"
     ]
    }
   ],
   "source": [
    "sarsa = CustomSARSA(policy_params, process_params, machine_params, environment)\n",
    "sarsa.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomQLearning : \n",
    "    def __init__(self, policy_params, process_params, machine_params, environment) : \n",
    "        self.policy_params, self.process_params, self.machine_params, self.environment = policy_params, process_params, machine_params, environment\n",
    "        self.reward_list, self.action_list, self.demand_list, self.not_list, self.inventory_list = [], [], [], [], []\n",
    "        \n",
    "        self.environment = environment\n",
    "        self.env = self.environment(self.process_params, self.machine_params) \n",
    "        \n",
    "        self.gamma, self.alpha, self.epsilon = self.policy_params['gamma'], self.policy_params['alpha'], self.policy_params['epsilon']\n",
    "        self.R_ba, self.q = 0, np.zeros((self.process_params['demand_space'][-1] * self.process_params['demand_space'][-1] + 1, \n",
    "                                         self.process_params['inventory_space'][-1] + 1, \n",
    "                                         self.process_params['max_day'] + 1,\n",
    "                                         self.process_params['action_space'][-1] + 1))\n",
    "    \n",
    "    def state_action_loc(self, S, A) : \n",
    "        return S['demand'], S['inventory'], A\n",
    "        \n",
    "    def state_value(self, S, A, optim = False) :\n",
    "        demand_loc, inventory_loc, action_loc = self.state_action_loc(S, A)\n",
    "        if optim : q = self.q[(demand_loc, inventory_loc, self.env.day + 1, action_loc)]\n",
    "        else : q = self.q[(demand_loc, inventory_loc, self.env.day, action_loc)]\n",
    "        #if optim : q = self.q[(demand_loc, inventory_loc, action_loc)]\n",
    "        #else : q = self.q[(demand_loc, inventory_loc, action_loc)]\n",
    "        #q = self.q[(demand_loc, inventory_loc, action_loc)]\n",
    "        return q\n",
    "    \n",
    "    def initialize(self) : \n",
    "        self.env = self.environment(self.process_params, self.machine_params)\n",
    "        return self.env.state\n",
    "    \n",
    "    def actor(self, S, optim = False) : \n",
    "        if not optim : \n",
    "            p = rd.random()\n",
    "            if p > self.epsilon : \n",
    "                return rd.randrange(self.process_params['action_space'][0], self.process_params['action_space'][-1])\n",
    "            else : \n",
    "                reward_list = [self.state_value(S, action) for action in range(self.process_params['action_space'][0], self.process_params['action_space'][-1])]\n",
    "                #print(reward_list)\n",
    "                #print(self.process_params['action_space'])\n",
    "                return np.argmax(reward_list)\n",
    "        else : \n",
    "            reward_list = [self.state_value(S, action, optim = True) for action in range(self.process_params['action_space'][0], self.process_params['action_space'][-1])]\n",
    "            #print(reward_list)\n",
    "            #print(self.process_params['action_space'])\n",
    "            return np.argmax(reward_list)\n",
    "    \n",
    "    def fit(self) : \n",
    "        for epoch in tqdm(range(0, self.policy_params['max_epoch'])) : \n",
    "            S = self.initialize()\n",
    "            current_reward, current_action, current_demand, current_inventory, current_not = [], [], [], [], []\n",
    "            stop = False\n",
    "            while not stop :\n",
    "                A = self.actor(S)\n",
    "                demand_loc, inventory_loc, action_loc = self.state_action_loc(S, A)\n",
    "                \n",
    "                if self.policy_params['verbose'] != None : \n",
    "                    if epoch%self.policy_params['verbose'] == 0 : \n",
    "                        print('Epoch {} | i {}'.format(epoch, self.env.day))\n",
    "                        print('Current State | demand {} | inventory {}'.format(self.env.demand, self.env.inventory))   \n",
    "                \n",
    "                R, stop = self.env.get_reward(A)\n",
    "                current_reward.append(R), current_action.append(A), current_demand.append(S['demand'])\n",
    "                \n",
    "                new_S = self.env.make_state()\n",
    "                \n",
    "                if self.policy_params['verbose'] != None : \n",
    "                    if epoch%self.policy_params['verbose'] == 0 : \n",
    "                        print('Result | Action {} | made {} | Reward {}'.format(A, self.env.made, R))\n",
    "                        print('Next State | demand {} | inventory {}'.format(self.env.demand, self.env.inventory))     \n",
    "                        \n",
    "                if stop : delta = R \n",
    "                else : \n",
    "                    max_A = self.actor(new_S, optim = True)\n",
    "                    delta = R + self.gamma * self.state_value(new_S, max_A, optim = True) - self.state_value(S, A)\n",
    "                    \n",
    "                if self.policy_params['verbose'] != None : \n",
    "                    if epoch%self.policy_params['verbose'] == 0 : \n",
    "                        if stop : print('next value {} | value {}'.format(0, self.state_value(S, A)))\n",
    "                        else : print('next value {} | value {}'.format(self.state_value(new_S, max_A, optim = True), self.state_value(S, A)))\n",
    "                        print('before : ', self.q[(demand_loc, inventory_loc, self.env.day)])\n",
    "                self.q[(demand_loc, inventory_loc, self.env.day, action_loc)] += self.alpha * delta\n",
    "                if self.policy_params['verbose'] != None : \n",
    "                    if epoch%self.policy_params['verbose'] == 0 : \n",
    "                        print('action {} | plus {}'.format(action_loc, self.alpha * delta))\n",
    "                        print('after : ', self.q[(demand_loc, inventory_loc, self.env.day)])\n",
    "                        print('=' * 40)\n",
    "                #self.q[(demand_loc, inventory_loc, action_loc)] += self.alpha * delta\n",
    "                #print(self.q)\n",
    "                \n",
    "                S = new_S\n",
    "                self.env.day += 1\n",
    "                \n",
    "            self.reward_list.append(current_reward), self.action_list.append(current_action), self.demand_list.append(current_demand), \n",
    "            self.not_list.append(np.mean(self.env.not_satisfied_list)), self.inventory_list.append(np.mean(self.env.inventory_list))\n",
    "            \n",
    "            if self.policy_params['verbose'] != None : \n",
    "                if epoch%self.policy_params['verbose'] == 0 : \n",
    "                    print(\"R mean {} | total action {} | total demand {}\".format(np.mean(current_reward), np.sum(current_action), np.sum(current_demand)))\n",
    "                    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30000/30000 [01:54<00:00, 262.54it/s]\n"
     ]
    }
   ],
   "source": [
    "ql = CustomQLearning(policy_params, process_params, machine_params, environment)\n",
    "ql.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomApproximateSARSA : \n",
    "    def __init__(self, policy_params, process_params, machine_params, environment) : \n",
    "        self.policy_params, self.process_params, self.machine_params, self.environment = policy_params, process_params, machine_params, environment\n",
    "        self.reward_list, self.action_list, self.demand_list, self.not_list, self.inventory_list = [], [], [], [], []\n",
    "        \n",
    "        self.environment = environment\n",
    "        self.env = self.environment(self.process_params, self.machine_params) \n",
    "        state_array = self.make_array(self.env.state)\n",
    "        \n",
    "        self.gamma, self.alpha, self.epsilon = self.policy_params['gamma'], self.policy_params['alpha'] * 1e-4, self.policy_params['epsilon']\n",
    "        self.R_ba, self.w = 0, np.zeros((self.process_params['demand_space'][-1] * self.process_params['demand_space'][-1] + 1, \n",
    "                                         self.process_params['inventory_space'][-1] + 1, \n",
    "                                         self.process_params['max_day'] + 1,\n",
    "                                         len(state_array) + 1))\n",
    "                                \n",
    "    \n",
    "    def make_array(self, state) : \n",
    "        state_array = np.array([state[k] for k in state.keys()])\n",
    "        state_array = pf(2).fit_transform(state_array.reshape([-1, 1]).T)[0]\n",
    "        return state_array\n",
    "    \n",
    "    def state_action_loc(self, S, A) : \n",
    "        return S['demand'], S['inventory'], A\n",
    "        \n",
    "    def state_value(self, S, A, optim = False) :\n",
    "        S_ = self.make_array(S)\n",
    "        S_A = np.append(S_, A)\n",
    "        if optim : return np.dot(self.w[S['demand'], S['inventory'], self.env.day + 1], S_A)\n",
    "        else : return np.dot(self.w[S['demand'], S['inventory'], self.env.day], S_A)\n",
    "    \n",
    "    def initialize(self) : \n",
    "        self.env = self.environment(self.process_params, self.machine_params)\n",
    "        return self.env.state\n",
    "    \n",
    "    def actor(self, S, optim = False) : \n",
    "        p = rd.random()\n",
    "        if p > self.epsilon : \n",
    "            return rd.randrange(self.process_params['action_space'][0], self.process_params['action_space'][-1])\n",
    "        elif not optim : \n",
    "            reward_list = [self.state_value(S, action) for action in range(self.process_params['action_space'][0], self.process_params['action_space'][-1])]\n",
    "            return np.argmax(reward_list)\n",
    "        else : \n",
    "            reward_list = [self.state_value(S, action, optim = True) for action in range(self.process_params['action_space'][0], self.process_params['action_space'][-1])]\n",
    "            return np.argmax(reward_list)\n",
    "    \n",
    "    def fit(self) : \n",
    "        for epoch in tqdm(range(0, self.policy_params['max_epoch'])) : \n",
    "            S = self.initialize()\n",
    "            A = self.actor(S)\n",
    "            current_reward, current_action, current_demand, current_inventory, current_not = [], [], [], [], []\n",
    "            stop = False\n",
    "            while not stop :\n",
    "                demand_loc, inventory_loc, action_loc = self.state_action_loc(S, A)\n",
    "                \n",
    "                if self.policy_params['verbose'] != None : \n",
    "                    if epoch%self.policy_params['verbose'] == 0 : \n",
    "                        print('Epoch {} | i {}'.format(epoch, self.env.day))\n",
    "                        print('Current State | demand {} | inventory {}'.format(self.env.demand, self.env.inventory))   \n",
    "                \n",
    "                R, stop = self.env.get_reward(A)\n",
    "                current_reward.append(R), current_action.append(A), current_demand.append(S['demand'])\n",
    "                \n",
    "                new_S = self.env.make_state()\n",
    "                \n",
    "                if self.policy_params['verbose'] != None : \n",
    "                    if epoch%self.policy_params['verbose'] == 0 : \n",
    "                        print('Result | Action {} | made {} | Reward {}'.format(A, self.env.made, R))\n",
    "                        print('Next State | demand {} | inventory {}'.format(self.env.demand, self.env.inventory))     \n",
    "                        \n",
    "                if stop : delta = R \n",
    "                else : \n",
    "                    new_A = self.actor(new_S, optim = True)\n",
    "                    delta = R + self.gamma * self.state_value(new_S, new_A, optim = True) - self.state_value(S, A)\n",
    "                    \n",
    "                if self.policy_params['verbose'] != None : \n",
    "                    if epoch%self.policy_params['verbose'] == 0 : \n",
    "                        if stop : print('next value {} | value {}'.format(0, self.state_value(S, A)))\n",
    "                        else : print('next value {} | value {}'.format(self.state_value(new_S, ), self.state_value(S, A)))\n",
    "                        print('before : ', self.q[(demand_loc, inventory_loc, self.env.day)])\n",
    "                #print(self.w[(demand_loc, inventory_loc, self.env.day)])\n",
    "                #print(self.alpha * delta)\n",
    "                #print(np.append(np.append(S, A), self.env.day))\n",
    "                S_ = self.make_array(S)\n",
    "                self.w[(demand_loc, inventory_loc, self.env.day)] += self.alpha * delta * np.append(S_, A)\n",
    "                if self.policy_params['verbose'] != None : \n",
    "                    if epoch%self.policy_params['verbose'] == 0 : \n",
    "                        print('action {} | plus {}'.format(action_loc, self.alpha * delta))\n",
    "                        print('after : ', self.q[(demand_loc, inventory_loc, self.env.day)])\n",
    "                        print('=' * 40)\n",
    "                #self.q[(demand_loc, inventory_loc, action_loc)] += self.alpha * delta\n",
    "                #print(self.q)\n",
    "                \n",
    "                S = new_S\n",
    "                A = new_A\n",
    "                self.env.day += 1\n",
    "                \n",
    "            self.reward_list.append(current_reward), self.action_list.append(current_action), self.demand_list.append(current_demand), \n",
    "            self.not_list.append(np.mean(self.env.not_satisfied_list)), self.inventory_list.append(np.mean(self.env.inventory_list))\n",
    "            \n",
    "            if self.policy_params['verbose'] != None : \n",
    "                if epoch%self.policy_params['verbose'] == 0 : \n",
    "                    print(\"R mean {} | total action {} | total demand {}\".format(np.mean(current_reward), np.sum(current_action), np.sum(current_demand)))\n",
    "                    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 28723/30000 [57:45<02:43,  7.80it/s] "
     ]
    }
   ],
   "source": [
    "a_sarsa = CustomApproximateSARSA(policy_params, process_params, machine_params, environment)\n",
    "a_sarsa.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = list(range(0, max_epoch))\n",
    "array = np.array([[np.mean(reward) for reward in sarsa.reward_list], \n",
    "                  [np.mean(reward) for reward in ql.reward_list], \n",
    "                  [np.mean(reward) for reward in a_sarsa.reward_list]])\n",
    "\n",
    "df = pd.DataFrame(array.T, columns = ['SARSA', 'Q-Learning', 'Approximate_SARSA'], index = index)\n",
    "df['SARSA_MA_6'] =  df['SARSA'].rolling(window = 6).mean()\n",
    "df['Q-Learning_MA_6'] = df['Q-Learning'].rolling(window = 6).mean()\n",
    "df['Approximate_SARSA_MA_6'] = df['Approximate_SARSA'].rolling(window = 6).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printing(df, columns, colors, sizes, scale, figsize = (20, 10), SAVE = None, SHOW = True) : \n",
    "    \n",
    "    passing = False \n",
    "    for c in columns : \n",
    "        if type(c) == str : \n",
    "            if len(df[~df[c].isna()]) == 0 : passing = True\n",
    "        elif type(c) == tuple : \n",
    "            for c_ in c : \n",
    "                if len(df[~df[c_].isna()]) == 0 : passing = True\n",
    "    \n",
    "    if passing : pass\n",
    "    else : \n",
    "        fig, ax = plt.subplots(figsize = figsize)\n",
    "\n",
    "        ax_list = [ax]\n",
    "        for i in range(1, len(columns)) : \n",
    "            ax_ = ax.twinx()\n",
    "            rspine = ax_.spines['right']\n",
    "            rspine.set_position(('axes', 0.95 + i * 0.05 ))\n",
    "            ax_.set_frame_on(True)\n",
    "            ax_.patch.set_visible(False)\n",
    "            ax_list.append(ax_)\n",
    "\n",
    "        fig.subplots_adjust(right = 0.8)\n",
    "\n",
    "        index = list(df.index)\n",
    "        \n",
    "        columns_ = []\n",
    "        total_line = None\n",
    "        for i, c in enumerate(columns) : \n",
    "            current_ax = ax_list[i]\n",
    "            if type(c) == str : \n",
    "                columns_.append(c)\n",
    "                \n",
    "                current = df[c]\n",
    "                current_std = np.std(current)\n",
    "                current_min = np.min(current) - current_std * scale[i]\n",
    "                current_max = np.max(current) + current_std * scale[i]\n",
    "                current_ax.set_ylim(current_min, current_max)\n",
    "                current_line = current_ax.plot(index, current, color = colors[i], label = c, marker = 'o', markersize = sizes[i])\n",
    "                \n",
    "                if i == 0 : total_line = current_line\n",
    "                else : total_line += current_line\n",
    "                    \n",
    "            elif type(c) == tuple :\n",
    "                current_min = 1e+1000000\n",
    "                current_max = -1e+1000000\n",
    "                for c_ in c : \n",
    "                    columns_.append(c_)\n",
    "                    \n",
    "                    current = df[c_]\n",
    "                    _std = np.std(current)\n",
    "                    _min = np.min(current) - _std * scale[i]\n",
    "                    _max = np.max(current) + _std * scale[i]\n",
    "                    if current_min > _min : current_min = _min\n",
    "                    if current_max < _max : current_max = _max\n",
    "                current_ax.set_ylim(current_min, current_max)\n",
    "                \n",
    "                for j, c_ in enumerate(c) : \n",
    "                    current = df[c_]\n",
    "                    current_line = current_ax.plot(index, current, color = colors[i][j], label = c_, marker = 'o', markersize = sizes[i][j])\n",
    "                    \n",
    "                    if i == 0 and j == 0 : total_line = current_line\n",
    "                    else : total_line += current_line\n",
    "                \n",
    "        plt.grid(True)\n",
    "        plt.legend(total_line, columns_)\n",
    "        fig.tight_layout()\n",
    "\n",
    "        if SAVE != None : plt.savefig(SAVE, dpi = 100)\n",
    "        if SHOW == True : plt.show()\n",
    "        \n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [('SARSA', 'Q-Learning', 'Approximate_SARSA')]\n",
    "colors = [('r', 'b', 'g')]\n",
    "sizes = [(5, 5, 5)]\n",
    "scale = [0.5]\n",
    "figsize = (14, 7)\n",
    "\n",
    "printing(df, columns, colors, sizes, scale, figsize = figsize, SAVE = None, SHOW = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [('SARSA_MA_6', 'Q-Learning_MA_6', 'Approximate_SARSA_MA_6')]\n",
    "colors = [('r', 'b', 'g')]\n",
    "sizes = [(5, 5, 5)]\n",
    "scale = [0.5]\n",
    "figsize = (14, 7)\n",
    "\n",
    "printing(df, columns, colors, sizes, scale, figsize = figsize, SAVE = '{}_complicated_profit.png'.format(max_epoch), SHOW = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
